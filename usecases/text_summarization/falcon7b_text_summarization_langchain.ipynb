{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d98e7379-6879-415c-856f-3ace919026d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Summarization on Custom Dataset with SageMaker Jumpstart and [LangChain](https://python.langchain.com/en/latest/index.html) Library\n",
    "\n",
    "Reference: https://github.com/gkamradt/langchain-tutorials/tree/main/data_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d4f450-2126-416f-a6a1-b3adfc14a921",
   "metadata": {},
   "source": [
    "\n",
    " There are two main types of methods for summarizing text: abstractive and extractive.\n",
    "\n",
    "Abstractive summarization generates a new shorter summary in its own words based on understanding the meaning and concepts of the original text. It analyzes the text using advanced natural language techniques to grasp the key ideas and then expresses those ideas in a summarized form using different words and phrases. This is similar to how humans summarize by reading something and then explaining the main points in their own words.\n",
    "\n",
    "Extractive summarization works by selecting the most important sentences, phrases or words from the original text to construct a summary. It calculates the weight or importance of each part of the text using algorithms and then chooses the parts with the highest weights to put into the summary. This pulls summarizes by extracting key elements from the text itself rather than interpreting the meaning.\n",
    "\n",
    "So in short, abstractive summarization rewrites the key ideas in new words while extractive summarization selects the most salient parts of the existing text. Both aim to distill the essence and most significant information from the original document into a condensed summary.\n",
    "\n",
    "We're going to run through 3 methods for summarization that start with basic prompting to summarizing large documents using `map_reduce` method. These aren't the only options, feel free to modify it based on your use case. \n",
    "\n",
    "**3 Levels Of Summarization:**\n",
    "1. **Summarize a couple sentences** - Basic Prompt\n",
    "2. **Summarize a couple paragraphs** - Prompt Templates\n",
    "3. **Summarize a large document with multiple pages** - Map Reduce\n",
    "\n",
    "In this notebook we will demonstrate how to use a **Falcon 7b Instruct** model for text summarization using a library of documents as a reference.\n",
    "\n",
    "**This notebook serves a template such that you can easily replace the example dataset by your own to build a custom text summarization application. Let's install some dependencies that will be required and initialize some basic variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def9c9e3-ab82-4be3-bfdf-8e1ca9bf3e98",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade sagemaker\n",
    "!pip install langchain\n",
    "!pip install datasets\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bcd837-3614-4a0d-a35b-f93a5131b2a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Note\n",
    "You must Restart Kernel here for the installations to take effect. After restarting kernel, run the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a95503-078d-4571-9939-6fa420e73466",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "print(f\"Region is {aws_region}, Role is {aws_role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e371d3e0-9666-4ade-a901-f508c2110d50",
   "metadata": {},
   "source": [
    "## Deploy large language model (LLM) and embedding model in SageMaker JumpStart\n",
    "---\n",
    "\n",
    "To better illustrate the idea, let's first deploy all the models that are required to perform the demo. You can see the list of Falcon models available via JumpStart by running the following code block. You can deploy any of the 7b models on a minimum of `ml.g5.12xlarge` instance type for ideal performance. For 40b we recommend atleast a 24xl or higher. In this tutorial, we will deploy the `huggingface-llm-falcon-7b-instruct-bf16` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c237850b-254c-4aa0-a05c-239e60608936",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To list all the available textgeneration models in JumpStart uncomment and run the code below\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models, list_jumpstart_tasks\n",
    "filter_value = \"task == llm\"\n",
    "\n",
    "print(\"===== Available Models =====\")\n",
    "text_generation_models = list_jumpstart_models(filter=filter_value)\n",
    "text_generation_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a98a0c-ad20-4a54-adfa-e92757564d86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = 'huggingface-llm-falcon-7b-instruct-bf16'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a308c990-abf9-4341-8014-975a40fc2f7b",
   "metadata": {},
   "source": [
    "We will now deploy this model to a SageMaker endpoint for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6018d429-cd3b-43cf-97ca-637362bdfdaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "try:\n",
    "    model = JumpStartModel(model_id=model_id, instance_type=\"ml.g5.2xlarge\")\n",
    "    predictor = model.deploy()\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118fe880-470f-4ae7-94e9-b908f910709a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name =predictor.endpoint_name\n",
    "region = aws_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d57e89-8748-453e-bf64-ed9721288b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"SageMaker Endpoint with Falcon-7b deployed: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317ff31d-d676-45c3-90f1-7c483dbf2e32",
   "metadata": {},
   "source": [
    "## Summarize a few sentences \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c922e936-9da3-4b7d-b56d-ac3c92af2e55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Given the following text, provide a coincise and complete summary.\n",
    "\n",
    "Text:\n",
    "Philosophy (from Greek: φιλοσοφία, philosophia, 'love of wisdom') \\\n",
    "is the systematized study of general and fundamental questions, \\\n",
    "such as those about existence, reason, knowledge, values, mind, and language. \\\n",
    "Some sources claim the term was coined by Pythagoras (c. 570 – c. 495 BCE), \\\n",
    "although this theory is disputed by some. Philosophical methods include questioning, \\\n",
    "critical discussion, rational argument, and systematic presentation.\n",
    "\n",
    "Summary:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9116c5b5-9dcd-459c-902d-9ebd06b4e6fb",
   "metadata": {},
   "source": [
    "In order to use our model endpoint with LangChain we wrap up endpoints for LLM into `langchain.llms.sagemaker_endpoint.SagemakerEndpoint` which is LangChain's built in support for SageMaker endpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58acef7-fda1-4a57-9dd7-f4777e4a67d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from langchain import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt,  \"parameters\": model_kwargs}) \n",
    "        return input_str.encode('utf-8')\n",
    "    \n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0][\"generated_text\"]\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "sm_llm=SagemakerEndpoint(\n",
    "        endpoint_name=endpoint_name, \n",
    "        region_name=aws_region,\n",
    "        model_kwargs={\"do_sample\": True,\n",
    "                                    \"top_p\": 0.9,\n",
    "                                    \"temperature\": 0.5,\n",
    "                                    \"max_new_tokens\":  100,\n",
    "                                    \"stop\": [\"<|endoftext|>\", \"</s>\"]},\n",
    "        content_handler=content_handler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e94fec1-88fb-4abf-8297-fafcafcd514f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_tokens = sm_llm.get_num_tokens(prompt)\n",
    "print (f\"Our prompt has {num_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ebd258-5276-4d81-b907-ae7e66fd315a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = sm_llm(prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3281803b-7889-416c-afbd-dc8cfebda57b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Given the following text, write a 1 line summary.\n",
    "\n",
    "Text:\n",
    "Philosophy (from Greek: φιλοσοφία, philosophia, 'love of wisdom') \\\n",
    "is the systematized study of general and fundamental questions, \\\n",
    "such as those about existence, reason, knowledge, values, mind, and language. \\\n",
    "Some sources claim the term was coined by Pythagoras (c. 570 – c. 495 BCE), \\\n",
    "although this theory is disputed by some. Philosophical methods include questioning, \\\n",
    "critical discussion, rational argument, and systematic presentation.\n",
    "\n",
    "Summary:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1e7bec-cf0d-45dc-88e0-03f381b496e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = sm_llm(prompt)\n",
    "print (output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ec95dd-4c5b-4db6-8a5c-ce077e9c9e39",
   "metadata": {},
   "source": [
    "##  Summarize a couple paragraphs -  Prompt Templates\n",
    "---\n",
    "\n",
    "Prompt templates are a great way to dynamically place text within your prompts. They are like [python f-strings](https://realpython.com/python-f-strings/) but specialized for working with language models.\n",
    "\n",
    "We're going to look at 2 short Paul Graham essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cd99d5-4fc5-478a-aac7-0b4212059973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"chromadb/paul_graham_essay\")\n",
    "essay1 = dataset['data'][0]['document']\n",
    "essay2 = dataset['data'][1]['document']\n",
    "\n",
    "essays=[essay1, essay2]\n",
    "for essay in essays:\n",
    "    print(essay)\n",
    "    print(\"===============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a80929-324f-44fb-8a7f-0b253863d8f9",
   "metadata": {},
   "source": [
    "Next let's create a prompt template which will hold our instructions and a placeholder for the essay. In this example we only want a 1 sentence summary to come back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c327a-0daf-4c76-94db-82c9a30b5e28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Given the following text, write a short summary.\n",
    "\n",
    "Text: {essay}\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"essay\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0418ab48-ad3e-445f-83ae-f0ac4bf1169a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_llm=SagemakerEndpoint(\n",
    "        endpoint_name=endpoint_name, \n",
    "        region_name=aws_region,\n",
    "        model_kwargs={\"do_sample\": True,\n",
    "                                    \"top_p\": 0.9,\n",
    "                                    \"temperature\": 0.8,\n",
    "                                    \"max_new_tokens\":  200,\n",
    "                                    \"stop\": [\"<|endoftext|>\", \"</s>\"]},\n",
    "        content_handler=content_handler\n",
    "    )\n",
    "\n",
    "for essay in essays:\n",
    "    summary_prompt = prompt.format(essay=essay)\n",
    "    \n",
    "    num_tokens = sm_llm.get_num_tokens(summary_prompt)\n",
    "    print (f\"--> This prompt + essay has {num_tokens} tokens\")\n",
    "    \n",
    "    summary = sm_llm(summary_prompt)\n",
    "    \n",
    "    print (f\"Summary: {summary.strip()}\")\n",
    "    print (\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b6a8c9-c9ce-4178-9474-095ed6ff6879",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Summarize large text  from multiple pages of a document - MapReduce\n",
    "---\n",
    "\n",
    "If you have multiple pages you'd like to summarize, you'll likely hve large amounts of text and will likely run into a token limit. Token limits won't always be a problem, but it is good to know how to handle them if you run into the issue.\n",
    "\n",
    "The chain type \"Map Reduce\" is a method that helps with this. You first generate a summary of smaller chunks (that fit within the token limit) and then you get a summary of the summaries.\n",
    "\n",
    "Check out [this video](https://www.youtube.com/watch?v=f9_BWhCI4Zo) for more information on how chain types work. We will use articles from the PubMed dataset available via HuggingFace `datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2404fbe2-5894-4429-b987-274cacbe855e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"ccdv/pubmed-summarization\")\n",
    "essay = dataset['train'][0]['article']\n",
    "print(essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf80a98-b756-4a14-885f-25a52afc9811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b991fc-b213-4d7f-a9c4-2347b13d8872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_llm.get_num_tokens(essay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b6fa0-d43d-47bf-9b31-efe1d58ed95e",
   "metadata": {},
   "source": [
    "That's too many, let's split our text up into chunks so they fit into the prompt limit. I'm going a chunk size of 2,000 characters. \n",
    "\n",
    "> You can think of tokens as pieces of words used for natural language processing. For English text, **1 token is approximately 4 characters** or 0.75 words. As a point of reference, the collected works of Shakespeare are about 900,000 words or 1.2M tokens.\n",
    "\n",
    "This means the number of tokens we should expect is 2,000 / 4 = ~500 token chunks. But this will vary, each body of text/code will be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4782136-d2bd-41c6-bf9d-78b5efb3e0f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\"], chunk_size=2000, chunk_overlap=500)\n",
    "\n",
    "docs = text_splitter.create_documents([essay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3186a918-89ea-478a-b958-46aae447bf1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_docs = len(docs)\n",
    "\n",
    "num_tokens_first_doc = sm_llm.get_num_tokens(docs[0].page_content)\n",
    "\n",
    "print (f\"Now we have {num_docs} documents and the first one has {num_tokens_first_doc} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d06706-a5b6-43e6-bd79-b77f7b159693",
   "metadata": {},
   "source": [
    "Great, assuming that number of tokens is consistent in the other docs we should be good to go. Let's use LangChain's [load_summarize_chain](https://python.langchain.com/en/latest/use_cases/summarization.html) method, we will use `refine` chain type for summarization. We first need to initialize our chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2072ddf5-4fdf-4df5-9ac5-503c06578c04",
   "metadata": {},
   "source": [
    "Our document is pretty large and has 19 chunks, so lets pick the first few chunks and try to summarize them using LangChain's load_summarize_chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95ff5b9-7339-4052-9e36-6ac2ee5383c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_chain = load_summarize_chain(llm=sm_llm, chain_type='map_reduce',\n",
    "                                     verbose=True # Set verbose=True if you want to see the prompts being used\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9807e202-ab77-4e5e-bfe6-e9244e3d2c7a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = summary_chain.run(docs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acc08a7-d9d7-4dc9-9abe-46e80a950820",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(output.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a35f4a-2e49-41ca-96bb-d7909ccc847c",
   "metadata": {},
   "source": [
    "---\n",
    "This summary is a great start, but since we took partial text our resulting summary isn't great and is left incomplete. This can be solved with a bit of prompt engineering but ideally we would like to summarize the whole document. So, lets modify to summarize the entire document and get only the key points as the final summary.\n",
    "\n",
    "In order to do this we will use custom prompts (like we did above) to instruct the model on what we need. But this time, instead of using just 5 chunks of the given document, we will use all chunks of the documents and use a MapReduce Summary chain from LangChain and our Falcon model hosted in SageMaker.\n",
    "\n",
    "We will Summarize the document using LangChain MapReduce summary chain\n",
    "\n",
    "- We will first generate summaries of the smaller chunks (map)\n",
    "- Then we will generate a narrative using the generated summaries (reduce)\n",
    "- Then we will use the shortened narrative to generate final key themes, summary of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce45dbd-72bd-4adc-957f-8b69d20987da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "llm =SagemakerEndpoint(\n",
    "        endpoint_name=endpoint_name, \n",
    "        region_name=aws_region,\n",
    "        model_kwargs={\"do_sample\": True,\n",
    "                                    \"top_p\": 0.9,\n",
    "                                    \"temperature\": 0.8,\n",
    "                                    \"max_new_tokens\":  100,\n",
    "                                    \"stop\": [\"<|endoftext|>\", \"</s>\"]},\n",
    "        content_handler=content_handler\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcc30af-9a69-44a9-83b7-f4c3ac538fd7",
   "metadata": {},
   "source": [
    "Let's define the Map chain that will generate summaries of each of the 30 chunks. In this case, you can see that it is just a regular LLMChain with a simple summary prompt. This is because we simply want to run summary on each of the indovidual chunks of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59da55b6-f062-4d31-8065-a3a403cef931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Map\n",
    "map_template = \"\"\"Given the following text, write a short summary.\n",
    "\n",
    "Text: {docs}\n",
    "Summary: \"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5714e10-6c35-4bb5-91bb-1d295520eba3",
   "metadata": {},
   "source": [
    "We then define the reduce chain. The purpose of this chain is to take all the generated summaries (by the map chain) and generate a single final summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985bf702-17c8-4890-b5a5-87ec20e2619c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reduce\n",
    "reduce_template = \"\"\"The following is set of summaries. Take these and distill it into a final, consolidated summary of the main themes. \n",
    "\n",
    "Text: {doc_summaries}\n",
    "Summary: \"\"\"\n",
    "\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391ce736-cfab-4dd6-94af-5fb56147b075",
   "metadata": {},
   "source": [
    "We then define a chain that combines all the generated summaries from the Map chain, subsequently pass it to the Reduce chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b959606-0d1f-44d8-a6a7-d401c1ce6abf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    ")\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0a0e9d-8526-426d-8452-cf9d131616f0",
   "metadata": {},
   "source": [
    "Finally we define the overall MapReduceDocumentsChain. This chain takes care of executing all the chains we have defined so far, passing the output(s) from one to the other to  generate the final summary. If you want to be able to see each of the steps as they execute, you can pass `verbose = True` in the `map_chain` and the `reduce_chain` initializations above. For this exercise we kept it default to False, but feel free to change it and execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffd55ca-6338-488a-9b03-215eeb5a9852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522c1dec-1592-4a27-abad-27cde3bbc1b4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we have already split our document into chunks previously so we will use it now\n",
    "print(map_reduce_chain.run(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303812af-67c8-4839-9b70-b16e4abf56fa",
   "metadata": {},
   "source": [
    "# Cleanup\n",
    "---\n",
    "\n",
    "We have seen how we can deploy a Falcon 7b Instruct model using SageMaker Endpoint and use it with LangChain to perform small text and very large text summarizations. Let's delete the endpoint to avoid incurring additional cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb41280-9d57-4991-9670-ad330fc0f43c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
